{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rNm1DdxIYnF",
        "outputId": "d9d15ed6-d320-4b48-e581-a0419d7659ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define function to calculate maximum number of images among all classes\n",
        "def calculate_max_images(data_dir):\n",
        "    max_images = 0\n",
        "    for image_class in os.listdir(data_dir):\n",
        "        class_dir = os.path.join(data_dir, image_class)\n",
        "        num_images = len(os.listdir(class_dir))\n",
        "        max_images = max(max_images, num_images)\n",
        "    return max_images\n",
        "\n",
        "\n",
        "# Define function to balance dataset with augmentation\n",
        "def balance_dataset_with_augmentation(data_dir):\n",
        "    max_images = calculate_max_images(data_dir)\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    for image_class in os.listdir(data_dir):\n",
        "        class_dir = os.path.join(data_dir, image_class)\n",
        "        images = os.listdir(class_dir)\n",
        "        num_images = len(images)\n",
        "        if num_images < max_images:\n",
        "            # Calculate number of augmentation steps needed\n",
        "            steps_needed = max_images - num_images\n",
        "            batch_size = min(32, steps_needed)  # Choose a reasonable batch size\n",
        "            steps_per_epoch = steps_needed // batch_size + 1\n",
        "\n",
        "            # Generate augmented images and save them\n",
        "            image_files = [os.path.join(class_dir, image) for image in images]\n",
        "            augmented_images = datagen.flow(np.array([load_img(img, target_size=(256,256)) for img in image_files]),\n",
        "                                            batch_size=batch_size,\n",
        "                                            save_to_dir=class_dir,\n",
        "                                            save_prefix='augmented_',\n",
        "                                            save_format='png')\n",
        "            for _ in range(steps_per_epoch):\n",
        "              augmented_images.next()\n",
        "\n",
        "\n",
        "# Specify the path to your dataset folder\n",
        "data_dir = '/content/drive/MyDrive/EM401/Augmentation/train'\n",
        "\n",
        "# Apply the balanced dataset with augmentation\n",
        "balance_dataset_with_augmentation(data_dir)"
      ]
    }
  ]
}