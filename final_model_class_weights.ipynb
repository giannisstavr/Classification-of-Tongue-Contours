{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nEbKTpI5z3H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.applications import ResNet50, InceptionV3, InceptionResNetV2, DenseNet121\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
        "from keras.models import Sequential\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters and hyperparemeters\n",
        "batch_size = 32\n",
        "in_epochs = 200\n",
        "fine_epochs = 50\n",
        "total_epochs = in_epochs + fine_epochs\n",
        "\n",
        "# ReduceLROnPlateau\n",
        "min_lr = 1e-10\n",
        "factor = 0.2\n",
        "\n",
        "#Optimizer 1\n",
        "learning_rate = 0.0001\n",
        "\n",
        "#Optimizer 2\n",
        "learning_rate2 = 0.00001\n",
        "\n",
        "# Early Stopping & Scheduler\n",
        "patience = 10\n",
        "\n",
        "# Early Stopping\n",
        "start_epoch = 50\n",
        "min_delta= 0.0001\n",
        "\n",
        "#Layers to unfreeze\n",
        "layers = 10\n",
        "# Classes\n",
        "class_names = ['front_bunched', 'front_up', 'mid_bunched', 'tip_up']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "#Image Size\n",
        "image_size = (256,256)\n",
        "\n",
        "# Directories\n",
        "data_dir = '/content/drive/MyDrive/EM401/Class_weights/Dataset'\n",
        "train_dir = '/content/drive/MyDrive/EM401/Class_weights/train'\n",
        "val_dir = '/content/drive/MyDrive/EM401/Class_weights/validation'\n",
        "test_dir = '/content/drive/MyDrive/EM401/Class_weights/test'\n",
        "\n",
        "# Create directories for train, validation, and test sets\n",
        "for directory in [train_dir, val_dir, test_dir]:\n",
        "    os.makedirs(directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "nvII1aoR55Fp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_images_per_class(data_dir):\n",
        "    class_counts = {}\n",
        "    total_images = 0\n",
        "\n",
        "    # Iterate over the subfolders in the main data directory\n",
        "    for image_class in os.listdir(data_dir):\n",
        "        # Check if the current item is a directory\n",
        "        if os.path.isdir(os.path.join(data_dir, image_class)):\n",
        "            # Count the number of images in the class\n",
        "            num_images = len(os.listdir(os.path.join(data_dir, image_class)))\n",
        "            # Store the count for the class\n",
        "            class_counts[image_class] = num_images\n",
        "            # Increment total_images count\n",
        "            total_images += num_images\n",
        "\n",
        "    return class_counts, total_images\n",
        "\n",
        "# Call the function to count images per class\n",
        "class_counts, total_images = count_images_per_class(data_dir)\n",
        "\n",
        "# Print the counts per class\n",
        "print(\"Number of images per class:\")\n",
        "for image_class, count in class_counts.items():\n",
        "    print(f\"{image_class}: {count}\")\n",
        "\n",
        "# Print the total number of images\n",
        "print(\"\\nTotal number of images:\", total_images)"
      ],
      "metadata": {
        "id": "fE74-p8RUtVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example of Calcualtion for Class weights (not needed)\n",
        "\n",
        "# Number of images per class\n",
        "num_front_bunched = 121\n",
        "num_front_up = 95\n",
        "num_tip_up = 114\n",
        "num_mid_bunched = 164\n",
        "\n",
        "# Total number of images\n",
        "total_images = num_front_bunched + num_front_up + num_tip_up + num_mid_bunched\n",
        "\n",
        "# Calculate class weights\n",
        "weight_front_bunched = total_images / num_front_bunched\n",
        "weight_front_up = total_images / num_front_up\n",
        "weight_tip_up = total_images / num_tip_up\n",
        "weight_mid_bunched = total_images / num_mid_bunched\n",
        "\n",
        "# Normalize weights\n",
        "total_weight = weight_front_bunched + weight_front_up + weight_tip_up + weight_mid_bunched\n",
        "weight_front_bunched /= total_weight\n",
        "weight_front_up /= total_weight\n",
        "weight_tip_up /= total_weight\n",
        "weight_mid_bunched /= total_weight\n",
        "\n",
        "\n",
        "print(\"Class Weights:\")\n",
        "print(\"front_bunched:\", weight_front_bunched)\n",
        "print(\"front_up:\", weight_front_up)\n",
        "print(\"tip_up:\", weight_tip_up)\n",
        "print(\"mid_bunched:\", weight_mid_bunched)\n",
        "\n"
      ],
      "metadata": {
        "id": "6WmeaDy-bPPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run once\n",
        "for image_class in os.listdir(data_dir):\n",
        "    class_train_dir = os.path.join(train_dir, image_class)\n",
        "    class_val_dir = os.path.join(val_dir, image_class)\n",
        "    class_test_dir = os.path.join(test_dir, image_class)\n",
        "    os.makedirs(class_train_dir, exist_ok=True)\n",
        "    os.makedirs(class_val_dir, exist_ok=True)\n",
        "    os.makedirs(class_test_dir, exist_ok=True)\n",
        "\n",
        "    images = os.listdir(os.path.join(data_dir, image_class))\n",
        "    train_size = int(len(images) * 0.7)\n",
        "    val_size = int(len(images) * 0.2)\n",
        "    train_images = images[:train_size]\n",
        "    val_images = images[train_size:train_size + val_size]\n",
        "    test_images = images[train_size + val_size:]\n",
        "\n",
        "    for image in train_images:\n",
        "        shutil.copy2(os.path.join(data_dir, image_class, image), os.path.join(class_train_dir, image))\n",
        "\n",
        "    for image in val_images:\n",
        "        shutil.copy2(os.path.join(data_dir, image_class, image), os.path.join(class_val_dir, image))\n",
        "\n",
        "    for image in test_images:\n",
        "        shutil.copy2(os.path.join(data_dir, image_class, image), os.path.join(class_test_dir, image))\n"
      ],
      "metadata": {
        "id": "z9hihMRD567p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming normalization is the main preprocessing step now\n",
        "train_datagen = ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    rescale=1./255  # Ensuring normalization\n",
        ")\n",
        "\n",
        "# For validation and test sets, apply only rescaling for normalization\n",
        "test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse'  # Assuming your labels are integer class indices\n",
        ")\n",
        "\n",
        "validation_generator = test_val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse',\n",
        "    shuffle=False  # Important for test set to evaluate in order\n",
        ")"
      ],
      "metadata": {
        "id": "hqhV5n9n6_xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract class labels from the training generator\n",
        "classes = train_generator.classes\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(classes),\n",
        "    y=classes)\n",
        "\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(\"Class Weights:\", class_weights)\n"
      ],
      "metadata": {
        "id": "rZeLTE711_E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model\n",
        "wResNet50 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-ResNet50_notop.h5'\n",
        "wIRV2 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-IRV2_notop.h5'\n",
        "wInceptionV3 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-InceptionV3_notop.h5'\n",
        "wDenseNet121 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-DenseNet121_notop.h5'\n",
        "\n",
        "base_model = InceptionV3(weights= wInceptionV3, include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create the model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)  # More units in the Dense layer\n",
        "x = Dropout(0.5)(x)  # Adjusted dropout rate\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "piZlF24K5_aB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning Rate scheduler 1\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience//2, min_lr=min_lr, verbose=1)"
      ],
      "metadata": {
        "id": "9Lw197nlztx-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sfcqaMJadbEd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "3bq5CBfDAOOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up callbacks\n",
        "logdir = '/content/drive/MyDrive/EM401/logs'\n",
        "os.makedirs(logdir, exist_ok=True)\n",
        "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, verbose=1, restore_best_weights=True, start_from_epoch=start_epoch)\n",
        "\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=in_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[tensorboard_callback, early_stopping, reduce_lr]\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "WAu9ORy-6DhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For fine-tuning, unfreeze the last set of layers and recompile the model\n",
        "for layer in base_model.layers[-layers:]:  # Experiment with the exact number of layers to unfreeze\n",
        "    layer.trainable = True\n",
        "\n",
        "# Switch to SGD with a low learning rate for fine-tuning\n",
        "from keras.optimizers import SGD, AdamW\n",
        "\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=learning_rate2),  # Lower learning rate for fine-tuning\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continue training the model with fine-tuning\n",
        "history_fine_tuning = model.fit(\n",
        "    train_generator,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=history.epoch[-1],\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n"
      ],
      "metadata": {
        "id": "HsvLDSOdR9Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test set\n",
        "evaluation = model.evaluate(test_generator)\n",
        "print(\"Test Accuracy:\", evaluation[1])\n",
        "print(\"Test Loss:\", evaluation[0])\n",
        "\n",
        "Save model\n",
        "save_directory = '/content/drive/MyDrive/EM401/models'\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "model.save(os.path.join(save_directory, 'final_model_class_weights.h5'))"
      ],
      "metadata": {
        "id": "8lRRAvcnpoVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation metrics NO FINE TUNE\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZF_K-TQApao2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot training and validation metrics FINE TUNE\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'] + history_fine_tuning.history['loss'], label='Training Loss (Fine-tuned)')\n",
        "plt.plot(history.history['val_loss'] + history_fine_tuning.history['val_loss'], label='Validation Loss (Fine-tuned)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'] + history_fine_tuning.history['accuracy'], label='Training Accuracy (Fine-tuned)')\n",
        "plt.plot(history.history['val_accuracy'] + history_fine_tuning.history['val_accuracy'], label='Validation Accuracy (Fine-tuned)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zpz7iTg46E3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# True labels\n",
        "true_classes = test_generator.classes\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "8IrFbC005tf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KWA3mmtq51pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(true_classes, predicted_classes, target_names=list(test_generator.class_indices.keys())))\n"
      ],
      "metadata": {
        "id": "ty7ASkFS54QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Predictions for validation as well\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(validation_generator)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# True labels\n",
        "true_classes = validation_generator.classes\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=validation_generator.class_indices, yticklabels=validation_generator.class_indices)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DyXZBAdepviB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}