{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHSBE6t_EqbF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.applications import ResNet50, InceptionV3, InceptionResNetV2, DenseNet121\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Parameters and hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 150\n",
        "\n",
        "# ReduceLROnPlateau\n",
        "min_lr = 1e-10\n",
        "factor = 0.2\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Early Stopping & Scheduler\n",
        "patience = 10\n",
        "\n",
        "# Early Stopping\n",
        "start_epoch = 50\n",
        "min_delta= 0.001\n",
        "\n",
        "# Classes\n",
        "class_names = ['front_bunched', 'front_up', 'mid_bunched', 'tip_up']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Image Size\n",
        "image_size = (256,256)\n",
        "\n",
        "# Directories\n",
        "data_dir = '/content/drive/MyDrive/EM401/SMOTE/Dataset'\n",
        "\n",
        "# Load input data\n",
        "def load_data(data_dir):\n",
        "    X, y = [], []\n",
        "    for class_name in os.listdir(data_dir):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(256, 256))\n",
        "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "            X.append(img)\n",
        "            y.append(class_name)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Split the dataset first\n",
        "X, y = load_data(data_dir)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Apply SMOTE only on the training data\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)\n",
        "\n",
        "# Normalize the data\n",
        "X_train_resampled = X_train_resampled.reshape(-1, 256, 256, 3) / 255.0\n",
        "X_val = X_val / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Convert class names to integer class indices\n",
        "class_indices = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "y_train_resampled = np.array([class_indices[label] for label in y_train_resampled])\n",
        "y_val = np.array([class_indices[label] for label in y_val])\n",
        "y_test = np.array([class_indices[label] for label in y_test])\n",
        "\n",
        "# Load base model\n",
        "wResNet50 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-ResNet50_notop.h5'\n",
        "wIRV2 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-IRV2_notop.h5'\n",
        "wInceptionV3 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-InceptionV3_notop.h5'\n",
        "wDenseNet121 = '/content/drive/MyDrive/EM401/models/RadImageNet_models/RadImageNet-DenseNet121_notop.h5'\n",
        "\n",
        "base_model = DenseNet121(weights=wDenseNet121, include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create the model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Set up callbacks\n",
        "logdir = '/content/drive/MyDrive/EM401/logs'\n",
        "os.makedirs(logdir, exist_ok=True)\n",
        "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, verbose=1, restore_best_weights=True, start_from_epoch=start_epoch)\n",
        "lr_scheduler = ReduceLROnPlateau(factor=factor, min_lr=min_lr, patience=patience//2, verbose=1)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train_resampled, y_train_resampled,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[tensorboard_callback, early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "evaluation = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", evaluation[1])\n",
        "print(\"Test Loss:\", evaluation[0])\n",
        "\n",
        "Save model\n",
        "save_directory = '/content/drive/MyDrive/EM401/models'\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "model.save(os.path.join(save_directory, 'final_model_smote.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CL7N-i1IEt0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "CghhhTRTHG5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "# Create a secondary y-axis for accuracy\n",
        "plt.twinx()\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "oNElEIivGdM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T7Dbdz4oEy1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred_classes, target_names=class_names)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "v587Rf24E1QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Load input data\n",
        "def load_data(data_dir):\n",
        "    X, y = [], []\n",
        "    for class_name in os.listdir(data_dir):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(256, 256))\n",
        "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "            X.append(img)\n",
        "            y.append(class_name)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "# Parameters and hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 150\n",
        "\n",
        "# Classes\n",
        "class_names = ['front_bunched', 'front_up', 'mid_bunched', 'tip_up']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Directories\n",
        "data_dir = '/content/drive/MyDrive/EM401/Images_Database_Unseen_18_03'\n",
        "\n",
        "# Load the data\n",
        "X, y = load_data(data_dir)\n",
        "\n",
        "\n",
        "\n",
        "# Iterate through each class folder\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    print(f\"Folder: {class_name}\")\n",
        "\n",
        "    # Iterate through images in the folder\n",
        "    for img_name in os.listdir(class_dir):\n",
        "        img_path = os.path.join(class_dir, img_name)\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(256, 256))\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.predict(img_array)\n",
        "        predicted_class = np.argmax(predictions)\n",
        "\n",
        "        print(f\"Image: {img_name}, Prediction: {class_names[predicted_class]}\")\n"
      ],
      "metadata": {
        "id": "s_nCfw_3073w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}